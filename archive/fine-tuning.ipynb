{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffebae7d",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6310b24d",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- Transfer learning from large pre-trained models to new small dataset \n",
    "    - Fine tuning existing model instead of training new model from scratch\n",
    "- Using transformers library can quickly fine tune BERT model (http://jalammar.github.io/illustrated-bert/)\n",
    "    - Google BERT (Bidirectional Embedding Representations from Transformers): semi-supervised training on large amounts of text (strong ability to recognized langauge patterns)\n",
    "        - Supervised usage: classify sentences (spam vs no spam), sentiment analysis \n",
    "    - Composed of large stack of encoders (1. self attention 2. feed forward NN)\n",
    "    - Putting sentences through BERT encodes it in specific vector count which can then be fed through a classifier or sentiment analysis?\n",
    "       - \"Vector hand off\" occurs between BERT and classifier\n",
    "    - Word embedding with Elmo (embeds based on context of word in sentence ex. 'nature' has multiple meanings, each unique)\n",
    "        - Needs to be trained on LARGE dataset then can be used as component in other models\n",
    "    - ULM-FiT has a process to fine-tune language models\n",
    "    - Fine-tuning JUST the decoder of a transformer\n",
    "    - Using BERT to create contextualized word embeddings THEN feed into existing \"model\" (what model?)\n",
    "- Transformer\n",
    "    - Uses \"attention\" to boost model speed\n",
    "    - Built of encoder (input) and decoder (output) stacks\n",
    "    - Flow:\n",
    "        - 1) Embedding algorithm (word2vec, GlOvE) turns input into vector/tensors (each word is a 512 size vector)\n",
    "        - 2) Builds more vectors for each word to understand the meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafb14e4",
   "metadata": {},
   "source": [
    "### Actions\n",
    "- Building sentiment analysis that looks at the ORDER of the words (https://nlp.stanford.edu/sentiment/code.html)\n",
    "    - Recursive NN built on top of grammatical structure\n",
    "    - Sentiment Treebank built by Stanford\n",
    "- Implementing Transformer w/ Pytorch (http://nlp.seas.harvard.edu/2018/04/03/attention.html)\n",
    "- BERT is too big to implement so trying ALBERT (https://medium.com/spark-nlp/1-line-to-albert-word-embeddings-with-nlu-in-python-1691bc048ed1)\n",
    "    - ALBERT uses two parameter reduction techniques making it quicker than BERT -> allowing for self-supervied learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "123c163d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests # download texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b3a329",
   "metadata": {},
   "source": [
    "## 1. Downloading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe49c41c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# download book data\n",
    "response1 = requests.get('http://classics.mit.edu/Antoninus/meditations.mb.txt') # Meditations by Marcus\n",
    "response2 = requests.get('http://classics.mit.edu/Epictetus/discourses.mb.txt') # Discourses by Epictetus\n",
    "response3 = requests.get('http://classics.mit.edu/Epictetus/epicench.1b.txt') # Enchiridion by Epictetus\n",
    "filename4 = open('/Users/stellajia/Desktop/UCSB/Fall-2022/ENGL197/stoic-analysis/data/shortness-of-life.txt') # Shortness of Life by Seneca\n",
    "filename5 = open('/Users/stellajia/Desktop/UCSB/Fall-2022/ENGL197/stoic-analysis/data/letters-from-a-stoic.txt') # Letters from a Stoic by Seneca\n",
    "\n",
    "doc1 = response1.text\n",
    "doc2 = response2.text\n",
    "doc3 = response3.text\n",
    "doc4 = filename4.read()\n",
    "doc5 = filename5.read()\n",
    "\n",
    "filename6 = open('/Users/stellajia/Desktop/UCSB/Fall-2022/ENGL197/stoic-analysis/data/daily-stoic.txt') # Daily Stoic by Ryan Holiday (2016)\n",
    "filename7 = open('/Users/stellajia/Desktop/UCSB/Fall-2022/ENGL197/stoic-analysis/data/guide-to-good-life.txt') # Guide to a Good Life by William Irvine (2008)\n",
    "filename8 = open('/Users/stellajia/Desktop/UCSB/Fall-2022/ENGL197/stoic-analysis/data/stillness-is-key.txt') # Stillness is Key by Ryan Holiday (2019)\n",
    "filenmae9 = open('/Users/stellajia/Desktop/UCSB/Fall-2022/ENGL197/stoic-analysis/data/how-to-be-a-stoic.txt') # How to Be a Stoic by Massimo Pigliucci (2017)\n",
    "filename10 = open('/Users/stellajia/Desktop/UCSB/Fall-2022/ENGL197/stoic-analysis/data/think-like-a-roman-emperor.txt') # How to Think Like a Roman Emperor by Donald Robertson (2019)\n",
    "\n",
    "doc6 = filename6.read() \n",
    "doc7 = filename7.read()\n",
    "doc8 = filename8.read()\n",
    "doc9 = filenmae9.read()\n",
    "doc10 = filename10.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6ba4216",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# download twitter data\n",
    "tweets = pd.read_csv('/Users/stellajia/Desktop/UCSB/Fall-2022/ENGL197/stoic-analysis/data/stoicism-tweets.csv') # from netlytic\n",
    "tweets2 = pd.read_excel('/Users/stellajia/Downloads/stoicism/stoicism-tweet.xls') # from socioviz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728bed21",
   "metadata": {},
   "source": [
    "## 2. Cleaning data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1babbac",
   "metadata": {},
   "source": [
    "### 2.1 Cleaning tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d00dbdd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove links from tweet\n",
    "tweets2_1 = []\n",
    "for i in range(len(tweets2)):\n",
    "    if 'https' in tweets2['Text']: \n",
    "        tweets2_1.append(tweets2['Text'][i][:-24])\n",
    "    else:\n",
    "        tweets2_1.append(tweets2['Text'][i])\n",
    "# tweets2_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f073545",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Western societies have become soft. The collap...\n",
       "1      All this fire at DeSantis and he hasn’t return...\n",
       "2      “Mean tweets” is funny when it’s banter. But t...\n",
       "3      @BillRevans So very sorry to hear this, and so...\n",
       "4      @SterlingSpector @womnofvalr Buddhism and Stoi...\n",
       "                             ...                        \n",
       "426    If you find something very difficult to achiev...\n",
       "427    Vanitas Still Life  by Pieter Claesz, 1630.  (...\n",
       "428    Contrary to what most \"Stoic guys\" say, Stoici...\n",
       "429    If you find something very difficult to achiev...\n",
       "430    Adversity Reveals - DAY 179 - The Daily Stoic ...\n",
       "Length: 531, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine tweets\n",
    "all_tweets = tweets2['Text'].append(tweets['description'])\n",
    "all_tweets.reset_index()\n",
    "all_tweets.drop(columns=['index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a48e01b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Western societies have become soft. The collap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>All this fire at DeSantis and he hasn’t return...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>“Mean tweets” is funny when it’s banter. But t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@BillRevans So very sorry to hear this, and so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@SterlingSpector @womnofvalr Buddhism and Stoi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>If you find something very difficult to achiev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>Vanitas Still Life  by Pieter Claesz, 1630.  (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>Contrary to what most \"Stoic guys\" say, Stoici...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>If you find something very difficult to achiev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>Adversity Reveals - DAY 179 - The Daily Stoic ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>531 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweet\n",
       "0    Western societies have become soft. The collap...\n",
       "1    All this fire at DeSantis and he hasn’t return...\n",
       "2    “Mean tweets” is funny when it’s banter. But t...\n",
       "3    @BillRevans So very sorry to hear this, and so...\n",
       "4    @SterlingSpector @womnofvalr Buddhism and Stoi...\n",
       "..                                                 ...\n",
       "426  If you find something very difficult to achiev...\n",
       "427  Vanitas Still Life  by Pieter Claesz, 1630.  (...\n",
       "428  Contrary to what most \"Stoic guys\" say, Stoici...\n",
       "429  If you find something very difficult to achiev...\n",
       "430  Adversity Reveals - DAY 179 - The Daily Stoic ...\n",
       "\n",
       "[531 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df = all_tweets.to_frame()\n",
    "tweets_df['tweet'] = tweets_df[0]\n",
    "tweets_df[['tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9849dddb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    “Mean tweets” is funny when it’s banter. But t...\n",
       "2    People talk a lot about #Stoicism but don\\'t n...\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tweets[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34c5968d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"People talk a lot about #Stoicism but don\\\\'t necessarily put it into practice in daily life. There are many Stoic exercises. I honestly think that for *some* people the key would be contemplating the View from Above regularly – it encapsulates the philosophy. - Donald Robertson\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['description'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec994a4",
   "metadata": {},
   "source": [
    "### 2.2 Cleaning books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "64541184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store books\n",
    "stoicDocs = [doc1, doc2, doc3, doc4, doc5, doc6, doc7, doc8, doc9, doc10]\n",
    "# turn list into string\n",
    "stoicString = \" \".join(stoicDocs)\n",
    "    \n",
    "# download stop words\n",
    "stop_words = open(\"/Users/stellajia/Desktop/UCSB/Fall-2022/ENGL197/stoic-analysis/data/buckley-salton.txt\").read()\n",
    "# remove stop words\n",
    "stoicWords = [word for word in stoicString.split() if word.lower() not in stop_words.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c3d24b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old length:  2810288\n",
      "New length:  207857\n"
     ]
    }
   ],
   "source": [
    "print(\"Old length: \", len(stoicString))\n",
    "print(\"New length: \", len(stoicWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21b67a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/11/16 20:22:20 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 805937 ms exceeds timeout 120000 ms\n",
      "22/11/16 20:22:20 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "# tokenize \n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81f3f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\") # pretrained model on English language using a masked language modeling\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = stoicWords.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96605cc",
   "metadata": {},
   "source": [
    "# STILL NEED TO CLEAN TWEETS!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ace40b",
   "metadata": {},
   "source": [
    "## 3. Using ALBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632601f0",
   "metadata": {},
   "source": [
    "### 3.1 First attempt - NLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e08bbf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlu # access to a bunch of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6fc37b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/stellajia/opt/miniconda3/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/stellajia/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/stellajia/.ivy2/jars\n",
      "com.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-8af00bd8-e44f-4196-9165-c1b5779ad33f;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;4.0.2 in central\n",
      "\tfound com.typesafe#config;1.4.2 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.29.5 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.828 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.code.findbugs#annotations;3.0.1 in central\n",
      "\tfound net.jcip#jcip-annotations;1.0 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.1 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.21 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.2 in central\n",
      "downloading https://repo1.maven.org/maven2/com/johnsnowlabs/nlp/spark-nlp_2.12/4.0.2/spark-nlp_2.12-4.0.2.jar ...\n",
      "\t[SUCCESSFUL ] com.johnsnowlabs.nlp#spark-nlp_2.12;4.0.2!spark-nlp_2.12.jar (1498ms)\n",
      "downloading https://repo1.maven.org/maven2/com/typesafe/config/1.4.2/config-1.4.2.jar ...\n",
      "\t[SUCCESSFUL ] com.typesafe#config;1.4.2!config.jar(bundle) (39ms)\n",
      "downloading https://repo1.maven.org/maven2/org/rocksdb/rocksdbjni/6.29.5/rocksdbjni-6.29.5.jar ...\n",
      "\t[SUCCESSFUL ] org.rocksdb#rocksdbjni;6.29.5!rocksdbjni.jar (1817ms)\n",
      "downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.828/aws-java-sdk-bundle-1.11.828.jar ...\n",
      "\t[SUCCESSFUL ] com.amazonaws#aws-java-sdk-bundle;1.11.828!aws-java-sdk-bundle.jar (6334ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/universal-automata/liblevenshtein/3.0.0/liblevenshtein-3.0.0.jar ...\n",
      "\t[SUCCESSFUL ] com.github.universal-automata#liblevenshtein;3.0.0!liblevenshtein.jar (23ms)\n",
      "downloading https://repo1.maven.org/maven2/com/navigamez/greex/1.0/greex-1.0.jar ...\n",
      "\t[SUCCESSFUL ] com.navigamez#greex;1.0!greex.jar (23ms)\n",
      "downloading https://repo1.maven.org/maven2/com/johnsnowlabs/nlp/tensorflow-cpu_2.12/0.4.2/tensorflow-cpu_2.12-0.4.2.jar ...\n",
      "\t[SUCCESSFUL ] com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.2!tensorflow-cpu_2.12.jar (10661ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/findbugs/annotations/3.0.1/annotations-3.0.1.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.findbugs#annotations;3.0.1!annotations.jar (20ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/protobuf/protobuf-java-util/3.0.0-beta-3/protobuf-java-util-3.0.0-beta-3.jar ...\n",
      "\t[SUCCESSFUL ] com.google.protobuf#protobuf-java-util;3.0.0-beta-3!protobuf-java-util.jar(bundle) (27ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/protobuf/protobuf-java/3.0.0-beta-3/protobuf-java-3.0.0-beta-3.jar ...\n",
      "\t[SUCCESSFUL ] com.google.protobuf#protobuf-java;3.0.0-beta-3!protobuf-java.jar(bundle) (56ms)\n",
      "downloading https://repo1.maven.org/maven2/it/unimi/dsi/fastutil/7.0.12/fastutil-7.0.12.jar ...\n",
      "\t[SUCCESSFUL ] it.unimi.dsi#fastutil;7.0.12!fastutil.jar (632ms)\n",
      "downloading https://repo1.maven.org/maven2/org/projectlombok/lombok/1.16.8/lombok-1.16.8.jar ...\n",
      "\t[SUCCESSFUL ] org.projectlombok#lombok;1.16.8!lombok.jar (74ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.21/slf4j-api-1.7.21.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.21!slf4j-api.jar (20ms)\n",
      "downloading https://repo1.maven.org/maven2/net/jcip/jcip-annotations/1.0/jcip-annotations-1.0.jar ...\n",
      "\t[SUCCESSFUL ] net.jcip#jcip-annotations;1.0!jcip-annotations.jar (19ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.1/jsr305-3.0.1.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.1!jsr305.jar (16ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/gson/gson/2.3/gson-2.3.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.gson#gson;2.3!gson.jar (21ms)\n",
      "downloading https://repo1.maven.org/maven2/dk/brics/automaton/automaton/1.11-8/automaton-1.11-8.jar ...\n",
      "\t[SUCCESSFUL ] dk.brics.automaton#automaton;1.11-8!automaton.jar (20ms)\n",
      ":: resolution report :: resolve 3096ms :: artifacts dl 21328ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.828 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.google.code.findbugs#annotations;3.0.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.1 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp_2.12;4.0.2 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.2 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.2 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tnet.jcip#jcip-annotations;1.0 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.29.5 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.21 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   17  |   17  |   17  |   0   ||   17  |   17  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-8af00bd8-e44f-4196-9165-c1b5779ad33f\n",
      "\tconfs: [default]\n",
      "\t17 artifacts copied, 0 already retrieved (572114kB/3231ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/16 17:29:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "albert_base_uncased download started this may take some time.\n",
      "Approximate size to download 42.7 MB\n",
      "[ | ]albert_base_uncased download started this may take some time.\n",
      "Approximate size to download 42.7 MB\n",
      "[ | ]Download done! Loading the resource.\n",
      "[ \\ ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-16 17:30:42.027131: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n",
      "sentence_detector_dl download started this may take some time.\n",
      "Approximate size to download 354.6 KB\n",
      "[ | ]sentence_detector_dl download started this may take some time.\n",
      "Approximate size to download 354.6 KB\n",
      "Download done! Loading the resource.\n",
      "[OK!]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>word_embedding_albert</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stoicism</td>\n",
       "      <td>[-1.0533853769302368, -0.8770667314529419, 0.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is</td>\n",
       "      <td>[1.0381802320480347, -1.4269936084747314, 0.93...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>about</td>\n",
       "      <td>[-0.38724860548973083, -1.548833966255188, 0.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>living</td>\n",
       "      <td>[-0.7819571495056152, -1.5227669477462769, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>in</td>\n",
       "      <td>[0.6698690056800842, -1.0752289295196533, 2.27...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tranquility</td>\n",
       "      <td>[-0.8311948776245117, -0.32697033882141113, 2....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         token                              word_embedding_albert\n",
       "0     Stoicism  [-1.0533853769302368, -0.8770667314529419, 0.8...\n",
       "0           is  [1.0381802320480347, -1.4269936084747314, 0.93...\n",
       "0        about  [-0.38724860548973083, -1.548833966255188, 0.8...\n",
       "0       living  [-0.7819571495056152, -1.5227669477462769, -0....\n",
       "0           in  [0.6698690056800842, -1.0752289295196533, 2.27...\n",
       "0  tranquility  [-0.8311948776245117, -0.32697033882141113, 2...."
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test on one sentence\n",
    "pipe = nlu.load('albert')\n",
    "pipe.predict('Stoicism is about living in tranquility')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9ac17c2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentimentdl_glove_imdb download started this may take some time.\n",
      "Approximate size to download 8.7 MB\n",
      "[ | ]sentimentdl_glove_imdb download started this may take some time.\n",
      "Approximate size to download 8.7 MB\n",
      "[ — ]Download done! Loading the resource.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n",
      "classifierdl_use_emotion download started this may take some time.\n",
      "Approximate size to download 21.3 MB\n",
      "[ | ]classifierdl_use_emotion download started this may take some time.\n",
      "Approximate size to download 21.3 MB\n",
      "[ \\ ]Download done! Loading the resource.\n",
      "[OK!]\n",
      "albert_base_uncased download started this may take some time.\n",
      "Approximate size to download 42.7 MB\n",
      "[OK!]\n",
      "tfhub_use download started this may take some time.\n",
      "Approximate size to download 923.7 MB\n",
      "[ | ]tfhub_use download started this may take some time.\n",
      "Approximate size to download 923.7 MB\n",
      "[ / ]Download done! Loading the resource.\n",
      "[OK!]\n",
      "glove_100d download started this may take some time.\n",
      "Approximate size to download 145.3 MB\n",
      "[ / ]glove_100d download started this may take some time.\n",
      "Approximate size to download 145.3 MB\n",
      "[OK!]\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Something went wrong during completing the DAG for the Spark NLP Pipeline.If this error persists, please contact us in Slack https://join.slack.com/t/spark-nlp/shared_invite/zt-lutct9gm-kuUazcyFKhuGY3_0AMkxqA Or open an issue on Github https://github.com/JohnSnowLabs/nlu/issues",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/nlu/pipe/component_resolution.py\u001b[0m in \u001b[0;36mget_trained_component_for_nlp_model_ref\u001b[0;34m(lang, nlu_ref, nlp_ref, license_type, model_configs)\u001b[0m\n\u001b[1;32m    275\u001b[0m             component = component.set_metadata(\n\u001b[0;32m--> 276\u001b[0;31m                 \u001b[0mcomponent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_pretrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_bucket\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m                 nlu_ref, nlp_ref, lang, False, license_type)\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/nlu/components/embeddings/glove/glove.py\u001b[0m in \u001b[0;36mget_pretrained_model\u001b[0;34m(name, language, bucket)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_pretrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbucket\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mWordEmbeddingsModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0msetInputCols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sentence\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"token\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/sparknlp/annotator/embeddings/word_embeddings.py\u001b[0m in \u001b[0;36mpretrained\u001b[0;34m(name, lang, remote_loc)\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0msparknlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrained\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mResourceDownloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mResourceDownloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloadModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWordEmbeddingsModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremote_loc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/sparknlp/pretrained/resource_downloader.py\u001b[0m in \u001b[0;36mdownloadModel\u001b[0;34m(reader, name, language, remote_loc, j_dwn)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                 \u001b[0mj_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_DownloadModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremote_loc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj_dwn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/sparknlp/internal/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, reader, name, language, remote_loc, validator)\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremote_loc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m         super(_DownloadModel, self).__init__(\"com.johnsnowlabs.nlp.pretrained.\" + validator + \".downloadModel\", reader,\n\u001b[0m\u001b[1;32m    318\u001b[0m                                              name, language, remote_loc)\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/sparknlp/internal/extended_java_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, java_obj, *args)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_java_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/sparknlp/internal/extended_java_wrapper.py\u001b[0m in \u001b[0;36mnew_java_obj\u001b[0;34m(self, java_class, *args)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnew_java_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjava_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_java_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_new_java_obj\u001b[0;34m(java_class, *args)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mjava_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mjava_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mjava_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Was not found appropriate resource to download for request: ResourceRequest(glove_100d,Some(en),public/models,4.0.2,3.3.1) with downloader: com.johnsnowlabs.nlp.pretrained.S3ResourceDownloader@744a84d3",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/nlu/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(request, path, verbose, gpu, streamlit_caching, m1_chip)\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mpipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipelineQueryVerifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_and_fix_nlu_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlu_ref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/nlu/pipe/pipe_logic.py\u001b[0m in \u001b[0;36mcheck_and_fix_nlu_pipeline\u001b[0;34m(pipe)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Satisfying dependencies'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mpipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipelineQueryVerifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msatisfy_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/nlu/pipe/pipe_logic.py\u001b[0m in \u001b[0;36msatisfy_dependencies\u001b[0;34m(pipe)\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmissing_component\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmissing_storage_refs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 component = resolve_feature(missing_component, language=pipe.lang,\n\u001b[0m\u001b[1;32m    321\u001b[0m                                             is_licensed=is_licensed, is_trainable_pipe=is_trainable)\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/nlu/pipe/component_resolution.py\u001b[0m in \u001b[0;36mresolve_feature\u001b[0;34m(missing_feature_type, language, is_licensed, is_trainable_pipe)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mlicense_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLicenses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhc\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_licensed\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mLicenses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_source\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mnlu_component\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_trained_component_for_nlp_model_ref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlu_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlp_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlicense_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnlu_component\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/nlu/pipe/component_resolution.py\u001b[0m in \u001b[0;36mget_trained_component_for_nlp_model_ref\u001b[0;34m(lang, nlu_ref, nlp_ref, license_type, model_configs)\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Failure making component, nlp_ref={nlp_ref}, nlu_ref={nlu_ref}, lang={lang}, \\n err={e}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Failure making component, nlp_ref=glove_100d, nlu_ref=en.embed.glove.100d, lang=en, \n err=requirement failed: Was not found appropriate resource to download for request: ResourceRequest(glove_100d,Some(en),public/models,4.0.2,3.3.1) with downloader: com.johnsnowlabs.nlp.pretrained.S3ResourceDownloader@744a84d3",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/3g/vjzcw6050n34kshmr3dgry980000gp/T/ipykernel_33629/3569810805.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# adding more models: part of speech, emotion, sentiment classifier, albert\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sentiment emotion albert'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'token'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/nlu/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(request, path, verbose, gpu, streamlit_caching, m1_chip)\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         raise Exception(f\"Something went wrong during completing the DAG for the Spark NLP Pipeline.\"\n\u001b[0m\u001b[1;32m    262\u001b[0m                         \u001b[0;34mf\"If this error persists, please contact us in Slack {slack_link} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m                         f\"Or open an issue on Github {github_issues_link}\")\n",
      "\u001b[0;31mException\u001b[0m: Something went wrong during completing the DAG for the Spark NLP Pipeline.If this error persists, please contact us in Slack https://join.slack.com/t/spark-nlp/shared_invite/zt-lutct9gm-kuUazcyFKhuGY3_0AMkxqA Or open an issue on Github https://github.com/JohnSnowLabs/nlu/issues"
     ]
    }
   ],
   "source": [
    "# adding more models: part of speech, emotion, sentiment classifier, albert\n",
    "pipe = nlu.load('sentiment emotion albert') \n",
    "predictions = pipe.predict(tweets_df[['tweet']], output_level='token')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d400ec5",
   "metadata": {},
   "source": [
    "### 3.2 Second attempt - johnsnowlabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54538050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlu not working so trying smth else\n",
    "from  johnsnowlabs import nlp # another way to access albert\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97240f91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/stellajia/opt/miniconda3/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/stellajia/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/stellajia/.ivy2/jars\n",
      "com.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-87db7851-9d56-4053-a474-7a1e9193c2b6;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;4.2.1 in central\n",
      "\tfound com.typesafe#config;1.4.2 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.29.5 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.828 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.code.findbugs#annotations;3.0.1 in central\n",
      "\tfound net.jcip#jcip-annotations;1.0 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.1 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.21 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.3 in central\n",
      ":: resolution report :: resolve 543ms :: artifacts dl 28ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.828 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.google.code.findbugs#annotations;3.0.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.1 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp_2.12;4.2.1 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.3 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.2 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tnet.jcip#jcip-annotations;1.0 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.29.5 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.21 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   17  |   0   |   0   |   0   ||   17  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-87db7851-9d56-4053-a474-7a1e9193c2b6\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 17 already retrieved (0kB/13ms)\n",
      "22/11/16 18:47:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentimentdl_glove_imdb download started this may take some time.\n",
      "Approximate size to download 8.7 MB\n",
      "[ | ]sentimentdl_glove_imdb download started this may take some time.\n",
      "Approximate size to download 8.7 MB\n",
      "Download done! Loading the resource.\n",
      "[ | ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-16 18:48:20.056762: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n",
      "glove_100d download started this may take some time.\n",
      "Approximate size to download 145.3 MB\n",
      "[ | ]glove_100d download started this may take some time.\n",
      "Approximate size to download 145.3 MB\n",
      "[ / ]Error: Unexpected end of ZLIB input stream\n",
      "Error: Unexpected end of ZLIB input stream\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/11/16 18:48:26 ERROR ResourceDownloader$: Unexpected end of ZLIB input stream\n",
      "22/11/16 18:48:26 ERROR ResourceDownloader$: Unexpected end of ZLIB input stream\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Something went wrong during completing the DAG for the Spark NLP Pipeline.If this error persists, please contact us in Slack https://join.slack.com/t/spark-nlp/shared_invite/zt-lutct9gm-kuUazcyFKhuGY3_0AMkxqA Or open an issue on Github https://github.com/JohnSnowLabs/nlu/issues",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/nlu/pipe/component_resolution.py\u001b[0m in \u001b[0;36mget_trained_component_for_nlp_model_ref\u001b[0;34m(lang, nlu_ref, nlp_ref, license_type, model_configs)\u001b[0m\n\u001b[1;32m    275\u001b[0m             component = component.set_metadata(\n\u001b[0;32m--> 276\u001b[0;31m                 \u001b[0mcomponent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_pretrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_bucket\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m                 nlu_ref, nlp_ref, lang, False, license_type)\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/nlu/components/embeddings/glove/glove.py\u001b[0m in \u001b[0;36mget_pretrained_model\u001b[0;34m(name, language, bucket)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_pretrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbucket\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mWordEmbeddingsModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0msetInputCols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sentence\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"token\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/sparknlp/annotator/embeddings/word_embeddings.py\u001b[0m in \u001b[0;36mpretrained\u001b[0;34m(name, lang, remote_loc)\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0msparknlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrained\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mResourceDownloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mResourceDownloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloadModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWordEmbeddingsModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremote_loc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/sparknlp/pretrained/resource_downloader.py\u001b[0m in \u001b[0;36mdownloadModel\u001b[0;34m(reader, name, language, remote_loc, j_dwn)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                 \u001b[0mj_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_DownloadModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremote_loc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj_dwn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/sparknlp/internal/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, reader, name, language, remote_loc, validator)\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremote_loc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m         super(_DownloadModel, self).__init__(\"com.johnsnowlabs.nlp.pretrained.\" + validator + \".downloadModel\", reader,\n\u001b[0m\u001b[1;32m    318\u001b[0m                                              name, language, remote_loc)\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/sparknlp/internal/extended_java_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, java_obj, *args)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_java_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/sparknlp/internal/extended_java_wrapper.py\u001b[0m in \u001b[0;36mnew_java_obj\u001b[0;34m(self, java_class, *args)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnew_java_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjava_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_java_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_new_java_obj\u001b[0;34m(java_class, *args)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mjava_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mjava_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mjava_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Was not found appropriate resource to download for request: ResourceRequest(glove_100d,Some(en),public/models,4.2.1,3.2.1) with downloader: com.johnsnowlabs.nlp.pretrained.S3ResourceDownloader@5834685a",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/nlu/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(request, path, verbose, gpu, streamlit_caching, m1_chip)\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mpipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipelineQueryVerifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_and_fix_nlu_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlu_ref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/nlu/pipe/pipe_logic.py\u001b[0m in \u001b[0;36mcheck_and_fix_nlu_pipeline\u001b[0;34m(pipe)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Satisfying dependencies'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mpipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipelineQueryVerifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msatisfy_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/nlu/pipe/pipe_logic.py\u001b[0m in \u001b[0;36msatisfy_dependencies\u001b[0;34m(pipe)\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmissing_component\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmissing_storage_refs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 component = resolve_feature(missing_component, language=pipe.lang,\n\u001b[0m\u001b[1;32m    321\u001b[0m                                             is_licensed=is_licensed, is_trainable_pipe=is_trainable)\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/nlu/pipe/component_resolution.py\u001b[0m in \u001b[0;36mresolve_feature\u001b[0;34m(missing_feature_type, language, is_licensed, is_trainable_pipe)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mlicense_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLicenses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhc\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_licensed\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mLicenses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_source\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mnlu_component\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_trained_component_for_nlp_model_ref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlu_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlp_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlicense_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnlu_component\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/nlu/pipe/component_resolution.py\u001b[0m in \u001b[0;36mget_trained_component_for_nlp_model_ref\u001b[0;34m(lang, nlu_ref, nlp_ref, license_type, model_configs)\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Failure making component, nlp_ref={nlp_ref}, nlu_ref={nlu_ref}, lang={lang}, \\n err={e}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Failure making component, nlp_ref=glove_100d, nlu_ref=en.embed.glove.100d, lang=en, \n err=requirement failed: Was not found appropriate resource to download for request: ResourceRequest(glove_100d,Some(en),public/models,4.2.1,3.2.1) with downloader: com.johnsnowlabs.nlp.pretrained.S3ResourceDownloader@5834685a",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/3g/vjzcw6050n34kshmr3dgry980000gp/T/ipykernel_38036/2668125193.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Wow that easy!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/nlu/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(request, path, verbose, gpu, streamlit_caching, m1_chip)\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         raise Exception(f\"Something went wrong during completing the DAG for the Spark NLP Pipeline.\"\n\u001b[0m\u001b[1;32m    262\u001b[0m                         \u001b[0;34mf\"If this error persists, please contact us in Slack {slack_link} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m                         f\"Or open an issue on Github {github_issues_link}\")\n",
      "\u001b[0;31mException\u001b[0m: Something went wrong during completing the DAG for the Spark NLP Pipeline.If this error persists, please contact us in Slack https://join.slack.com/t/spark-nlp/shared_invite/zt-lutct9gm-kuUazcyFKhuGY3_0AMkxqA Or open an issue on Github https://github.com/JohnSnowLabs/nlu/issues"
     ]
    }
   ],
   "source": [
    "nlp.load('sentiment').predict('Wow that easy!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44055e6b",
   "metadata": {},
   "source": [
    "### 3.3 Third attempt - transformers\n",
    "- Downloading a deep learning library (Pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6e4bb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-16 19:09:56.260485: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4839b3b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.9977403879165649}]\n"
     ]
    }
   ],
   "source": [
    "print(pipeline('sentiment-analysis')('statistics is hard to learn'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22783c2f",
   "metadata": {},
   "source": [
    "## 4. Self-supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad049d5",
   "metadata": {},
   "source": [
    "- Self supervised sentiment analyzer - https://www.sciencedirect.com/science/article/pii/S2666827021000074\n",
    "    - \"SSentiA utilizing limited labeled data can yield similar performance to a fully labeled training dataset\"\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
